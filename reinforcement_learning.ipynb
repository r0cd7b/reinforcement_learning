{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7460f1b0-7d9d-48b2-b7b4-19ff3bd5b62d",
   "metadata": {
    "id": "7460f1b0-7d9d-48b2-b7b4-19ff3bd5b62d"
   },
   "source": [
    "# 강화 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b50feb-a8db-4c0c-9f9a-afa522f19cc6",
   "metadata": {
    "id": "89b50feb-a8db-4c0c-9f9a-afa522f19cc6"
   },
   "source": [
    "먼저 몇 개의 모듈을 임포트한다. 맷플롯립 그림을 저장하는 함수를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7630c886-2e78-486e-9ae2-98e94616dc65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7630c886-2e78-486e-9ae2-98e94616dc65",
    "outputId": "4669aa79-394b-4164-b957-05a3fa6946f9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 06:46:19.214783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-02 06:46:20.335952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-02 06:46:20.348373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-02 06:46:20.348611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if not tf.config.list_physical_devices(\"GPU\"):\n",
    "    print(\"감지된 GPU가 없습니다. GPU가 없으면 LSTM과 CNN이 매우 느릴 수 있습니다.\")\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc(\"axes\", labelsize=14)\n",
    "mpl.rc(\"xtick\", labelsize=12)\n",
    "mpl.rc(\"ytick\", labelsize=12)\n",
    "\n",
    "# 부드러운 애니메이션을 위해\n",
    "\n",
    "mpl.rc(\"animation\", html=\"jshtml\")\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장 \" + fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, dpi=resolution, format=fig_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214800f",
   "metadata": {},
   "source": [
    "## OpenAI 짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd47ccf",
   "metadata": {},
   "source": [
    "이 노트북은 강화학습 알고리즘을 개발하고 평가하는 훌륭한 도구인 [OpenAI 짐(gym)](https://gym.openai.com/)을 사용한다. 학습 에이전트가 상호작용하기 위한 환경을 많이 제공한다. 먼저 `gym`을 임포트한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d105095-8dc8-4c5f-93ed-ff63c2d24b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485bddc",
   "metadata": {},
   "source": [
    "가능한 환경 목록을 확인한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5907e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([EnvSpec(id='CartPole-v0', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0), EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1), EnvSpec(id='MountainCar-v0', entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0), EnvSpec(id='MountainCarContinuous-v0', entry_point='gym.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0), EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1), EnvSpec(id='Acrobot-v1', entry_point='gym.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1), EnvSpec(id='LunarLander-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2), EnvSpec(id='LunarLanderContinuous-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2), EnvSpec(id='BipedalWalker-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3), EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3), EnvSpec(id='CarRacing-v2', entry_point='gym.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2), EnvSpec(id='Blackjack-v1', entry_point='gym.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1), EnvSpec(id='FrozenLake-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1), EnvSpec(id='FrozenLake8x8-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1), EnvSpec(id='CliffWalking-v0', entry_point='gym.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0), EnvSpec(id='Taxi-v3', entry_point='gym.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3), EnvSpec(id='Reacher-v2', entry_point='gym.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2), EnvSpec(id='Reacher-v4', entry_point='gym.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4), EnvSpec(id='Pusher-v2', entry_point='gym.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2), EnvSpec(id='Pusher-v4', entry_point='gym.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4), EnvSpec(id='InvertedPendulum-v2', entry_point='gym.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2), EnvSpec(id='InvertedPendulum-v4', entry_point='gym.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4), EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2), EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gym.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4), EnvSpec(id='HalfCheetah-v2', entry_point='gym.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2), EnvSpec(id='HalfCheetah-v3', entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3), EnvSpec(id='HalfCheetah-v4', entry_point='gym.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4), EnvSpec(id='Hopper-v2', entry_point='gym.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2), EnvSpec(id='Hopper-v3', entry_point='gym.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3), EnvSpec(id='Hopper-v4', entry_point='gym.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4), EnvSpec(id='Swimmer-v2', entry_point='gym.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2), EnvSpec(id='Swimmer-v3', entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3), EnvSpec(id='Swimmer-v4', entry_point='gym.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4), EnvSpec(id='Walker2d-v2', entry_point='gym.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2), EnvSpec(id='Walker2d-v3', entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3), EnvSpec(id='Walker2d-v4', entry_point='gym.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4), EnvSpec(id='Ant-v2', entry_point='gym.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2), EnvSpec(id='Ant-v3', entry_point='gym.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3), EnvSpec(id='Ant-v4', entry_point='gym.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4), EnvSpec(id='Humanoid-v2', entry_point='gym.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2), EnvSpec(id='Humanoid-v3', entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3), EnvSpec(id='Humanoid-v4', entry_point='gym.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4), EnvSpec(id='HumanoidStandup-v2', entry_point='gym.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2), EnvSpec(id='HumanoidStandup-v4', entry_point='gym.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a7819",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Cart-Pole은 매우 간단한 환경으로 왼쪽과 오른쪽으로 움직이는 카트와 그 위에 수직으로 놓여 있는 막대로 구성된다. 에이전트는 카트를 왼쪽이나 오른쪽으로 움직여 막대가 바로 서 있도록 만들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b32b650",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5afce",
   "metadata": {},
   "source": [
    "`reset()` 메서드를 호출해 환경을 초기화한다. 이 메서드는 관측을 반환한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bd7f87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a0c95",
   "metadata": {},
   "source": [
    "관측은 환경에 따라 다르다. 이 경우 4개의 실수로 구성된 1D 넘파이 배열이다. 카트의 수평 위치, 속도, 막대의 각도(0=수직), 각속도를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a349014",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a4627",
   "metadata": {},
   "source": [
    "환경은 `render()` 메서드를 호출하여 시각화할 수 있다. 그리고 렌더링 모드(환경에 따른 렌더링 옵션)를 선택할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0df68",
   "metadata": {},
   "source": [
    "**경고**: (Cart-Pole을 포함해) 일부 환경은 화면 접근 권한이 필요하다. `render_mode=\"rgb_array\"`로 지정하더라도 별도의 윈도우를 연다. 일반적으로 이 윈도우를 무시할 수 있다. 하지만 주피터를 백엔드(headless) 서버로 실행한다면 예외가 발생한다. 이를 피하는 한 가지 방법은 [Xvfb](http://en.wikipedia.org/wiki/Xvfb) 같은 가짜 X 서버를 설치하는 것이다. 데비안이나 우분투에서는 다음과 같이 설치한다:\n",
    "\n",
    "```bash\n",
    "$ apt update\n",
    "$ apt install -y xvfb\n",
    "```\n",
    "\n",
    "그다음 `xvfb-run` 명령으로 주피터를 실행한다:\n",
    "\n",
    "```bash\n",
    "$ xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook\n",
    "```\n",
    "\n",
    "또는 Xvfb를 감싼 [pyvirtualdisplay](https://github.com/ponty/pyvirtualdisplay) 파이썬 라이브러리를 설치할 수 있다:\n",
    "\n",
    "```bash\n",
    "%pip install -U pyvirtualdisplay\n",
    "```\n",
    "\n",
    "그다음 다음 코드를 실행한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43b5ab45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyvirtualdisplay\n",
    "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970ba4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f0b77",
   "metadata": {},
   "source": [
    "이 예에서는 `render_mode=\"rgb_array\"`로 지정해 환경 이미지를 넘파이 배열로 받는다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e87cb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e2fe96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_environment(env, figsize=(5, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59160b7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_environment(env)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287522bc",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "환경과 상호작용하는 방법을 알아 본다. 에이전트는 \"행동 공간\"(가능한 행동의 집합)에서 하나의 행동을 선택해야 한다. 이 환경의 행동 공간을 다음처럼 확인한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181684a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddef164",
   "metadata": {},
   "source": [
    "단 두 개의 행동이 가능하다: 왼쪽 또는 오른쪽으로 가속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626c5ba",
   "metadata": {},
   "source": [
    "막대가 오른쪽으로 기울어져 있기 때문에(`obs[2] > 0`), 카트를 오른쪽으로 가속한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bf6ff21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1  # 오른쪽으로 가속\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef88a8",
   "metadata": {},
   "source": [
    "이제 카트가 오른쪽으로 움직였다(`obs[1] > 0`). 막대가 여전히 오른쪽으로 기울었다(`obs[2] > 0`). 하지만 각속도가 음수이므로(`obs[3] < 0`) 다음 스텝에서는 왼쪽으로 기울 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d4a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그림 저장 cart_pole_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFNCAYAAADPdCxsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJ7klEQVR4nO3dQYtdZx3H8ec592bStKmkNdCKpVAXQgm6aDfFnRS6dCPqG3HtwtfQN+DaRUFwUwq1WDHpRlyIxtbSYgRrW01qzHTm3nseFwFBSu5MZqZzf+c5nw9kkzl3+G/OfDnPee45tbXWCgAQadj1AADAgwk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAsOWuBwDOXmtjKa1tP6gOpdZ6PgMBJybU0JnWWvnHH94qf/3tz7ced+0HPymXrjx9TlMBJyXU0J1Wxs2qtHF9xGFHXHEDEdyjht60VtrmiEgDkyHU0JnWWmnjZtdjAGdEqKE3bXRFDR0RauiMK2roi1BDd1oZj9pIBkyGUENvWjt6xzcwGUINnWmtlbax9A29EGroTWtltJkMuiHU0JnWRkvf0BGhhu7Y9Q09EWrozGZ1UA4++3jrMXuPXy3Dcu+cJgJOQ6ihM5vD/bL/z79tPeaxq8+WxYWL5zQRcBpCDTNUF8tSvOISJkGoYYbqYllKEWqYAqGGGRqGZamuqGEShBpmyNI3TIdQwwzVwdI3TIVQwwwNC0vfMBVCDTNUh4Wlb5gIoYYZGuz6hskQapihaukbJkOoYYbqYNc3TIVQwwxZ+obpEGroSGuttM0x3pxVq6VvmAihhs60jXdRQ0+EGjozbla7HgE4Q0INnRFq6ItQQ2eEGvoi1NCZJtTQFaGGzoxroYaeCDV0pVn6hs4INXTGFTX0RaihM+5RQ1+EGjozeuAJdEWooSetlM3hve3H1KHUujifeYBTE2roSGtj+eRPb2895tEnv14e/eoz5zQRcFpCDXMzDPf/AZPgbIWZqXVh6RsmRKhhZuowlOqKGibD2QozU4dFqYMrapgKoYaZqXUotTr1YSqcrTA31WYymBJnK8zM/XvUlr5hKoQaZqYOC0vfMCHOVpiZWu36hilxtsLM1MEjRGFKhBrmpi5cUcOEOFuhJ60deUgdhvs7v4FJcLZCR8bxeK+4rLV+yZMAZ0WooSNts9r1CMAZE2royLheHWv5G5gOoYaOjJtVkWnoi1BDRyx9Q3+EGjoyroUaeiPU0JFxc7xd38B0CDV0xNI39EeooSOWvqE/Qg0dGV1RQ3eEGjpyP9S+oAU9EWroiKVv6I9QQ0c+ufn21ieTLS4+Vp547oVznAg4LaGGnozj1h/XWsuw3DunYYCzINQwK7XUYbHrIYCHINQwJ7WWYbHc9RTAQxBqmJFaa6mLC7seA3gIQg2zUstg6RsmRahhTmop1dI3TIpQw4zUUoUaJkaoYU5sJoPJEWqYk1pLHYQapkSoYUZqEWqYGqGGTrQtjw79n1pLXdj1DVMi1NCL1o713ixfz4JpEWroRBvXpbTtz/oGpkeooRPjZrP1zVnANAk1dKKN6+PdpwYmRaihE21cl3Ksu9TAlAg1dGLcbFxRQ4eEGjphMxn0SaihE220mQx6JNTQibZZl+N9kxqYEqGGTtxf+hZq6I1QQyfubyZzjxp6I9TQCVfU0Cehhk785+MPy/rg3tZjvvLMtXOaBjgr3ncHIdbrdbl79+6JP3/3Xx+VtlltP+jSE+X27dsn+v3L5bJcvnz5RJ8FTq42T0iACDdu3CgvvfTSiT//4x99p/zwuw++Ym6tlZ/+7K3yy+vvnuj3v/zyy+WNN9446XjACVn6hhk5WG12PQLwkCx9Q0daK+WwPVI+2P9W+ff6ybIcDsvTe++Xp/Y+LKWUcrha73hC4GEJNXTk8/Gx8pvb3y/rtldaqaWUUj46eK48d+n35RuXflc+d0UNkyPU0InP1lfLjTvfK6v2yP/9/1iW5S/7L5ZSWjlY/WI3wwEn5h41dOLWwTfL/vj4A3/+3r0Xyv7hOQ4EnAmhhhmxmQymR6hhRoQapkeooRNPLv9eLtT9B/78qb0Pympl7RumRqihE09ffL98+/FflVq++GKOr+29V65d/nVZr4Uapsaub+jE54fr8sjqvXLtwp3y7r0Xy2frq2U5HJRnLv65PDv8sRzsH5TDtaVvmJpjh/r111//MueA2bt58+apPv/qa++UV19754ym+aJPP/3U3wE4Q6+88sqxjjt2qK9fv37iYYCj3bp1a9cjbHXnzh1/B+AMHTfUXsoBIU77Uo4vm5dywG7YTAYAwYQaAIIJNQAEE2oACCbUABBMqAEgmFADQDChBoBgQg0AwYQaAIJ5exaEeP7558ubb7656zEe6MqVK7seAWbJs74BIJilbwAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASCYUANAMKEGgGBCDQDBhBoAggk1AAQTagAIJtQAEEyoASDYfwGKw6g9fWLuHwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_environment(env)\n",
    "save_fig(\"cart_pole_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4788ebe",
   "metadata": {},
   "source": [
    "요청한 대로 실행된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90b1aa",
   "metadata": {},
   "source": [
    "환경은 이전 스텝에서 얼마나 많은 보상을 받는지 에이전트에게 알린다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff5861ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358015a",
   "metadata": {},
   "source": [
    "게임이 끝나면 환경은 `terminated=True`를 반환한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "586c0b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1100955",
   "metadata": {},
   "source": [
    "마지막으로 `info`는 훈련이나 디버깅에 유용한 추가적인 정보를 담은 환경에 특화된 딕셔너리이다. 예를 들어 일부 게임에서는 얼마나 많은 에이전트의 생명이 몇 개가 남아 있는지 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8659f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2cf2cd",
   "metadata": {},
   "source": [
    "환경이 재설정된 순간부터 종료될 때까지 스텝 시퀀스를 \"에피소드\"라고 한다. 에피소드 끝에서(즉, `step()`이 `terminated=True`를 반환할 때), 계속하기 전에 환경을 재설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0f7d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "if terminated:\n",
    "    obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd8037",
   "metadata": {},
   "source": [
    "그럼 어떻게 막대를 똑바로 유지할 수 있을까? 이를 위해 정책을 정의해야 한다. 에이전트가 매 스텝마다 행동을 선택하기 위해 사용할 전략이다. 어떤 행동을 선택할지 결정하기 위해 지난 행동과 관측을 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e978c8e",
   "metadata": {},
   "source": [
    "**간단한 하드 코딩 정책**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359772a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
